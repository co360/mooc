{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web predictions\n",
    "\n",
    "The purpose of this notebook is to experiment with making predictions from \"raw\" accumulated user values, that\n",
    "could for instance be user input from a web form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('sparkify-capstone-web').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "No such struct field rawCount in id, prediction, impurity, impurityStats, gain, leftChild, rightChild, split;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-caaf023a8279>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformedPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictionsPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGBTClassificationModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"out/model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\dev\\runtimes\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\runtimes\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mjava_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_from_java\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[1;32mC:\\dev\\runtimes\\spark-3.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\runtimes\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\runtimes\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: No such struct field rawCount in id, prediction, impurity, impurityStats, gain, leftChild, rightChild, split;"
     ]
    }
   ],
   "source": [
    "transformedPath = \"out/transformed.parquet\"\n",
    "predictionsPath = \"out/predictions.parquet\"\n",
    "df_transformed = spark.read.parquet(transformedPath)\n",
    "df_predictions = spark.read.parquet(predictionsPath)\n",
    "model = GBTClassificationModel.load(\"out/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones: 93, Zeros: 355\n",
      "26.197183098591548\n"
     ]
    }
   ],
   "source": [
    "zeros = df_predictions.filter(df_predictions[\"prediction\"] == 0)\n",
    "ones = df_predictions.filter(df_predictions[\"prediction\"] == 1)\n",
    "zerosCount = zeros.count()\n",
    "onesCount = ones.count()\n",
    "print(\"Ones: {}, Zeros: {}\".format(onesCount, zerosCount))\n",
    "print(onesCount / zerosCount * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPredictedToChurn = df_predictions.filter(df_predictions[\"prediction\"] == 1).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "296\n",
      "100003\n",
      "200021\n",
      "100042\n"
     ]
    }
   ],
   "source": [
    "for row in usersPredictedToChurn:\n",
    "    print(int(row[\"userId\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+------------+-------------+---------------+------------+-------------+-----------------+---------------+\n",
      "|userId|churn|level_index|gender_index|thumbs_up_sum|thumbs_down_sum|nextsong_sum|downgrade_sum|       length_sum|sessionId_count|\n",
      "+------+-----+-----------+------------+-------------+---------------+------------+-------------+-----------------+---------------+\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        1.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        1.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        1.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        1.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "+------+-----+-----------+------------+-------------+---------------+------------+-------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|  userId|\n",
      "+----------+--------+\n",
      "|       0.0|   148.0|\n",
      "|       0.0|200049.0|\n",
      "|       0.0|300040.0|\n",
      "|       1.0|    85.0|\n",
      "|       0.0|   137.0|\n",
      "|       0.0|   251.0|\n",
      "|       0.0|200031.0|\n",
      "|       0.0|300044.0|\n",
      "|       0.0|    65.0|\n",
      "|       0.0|200001.0|\n",
      "|       0.0|    53.0|\n",
      "|       0.0|   255.0|\n",
      "|       0.0|   133.0|\n",
      "|       1.0|   296.0|\n",
      "|       1.0|100003.0|\n",
      "|       1.0|200021.0|\n",
      "|       0.0|    78.0|\n",
      "|       0.0|100007.0|\n",
      "|       1.0|100042.0|\n",
      "|       0.0|100035.0|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 300044\n",
    "# 0 251\n",
    "\n",
    "# Select the prediction of a user as value\n",
    "pred = df_predictions[df_predictions[\"userId\"] == 78].select(\"prediction\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\runtimes\\spark-2.4.6-bin-hadoop2.7\\python\\pyspark\\sql\\session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "# From a query that could be entered in a web form, create a prediction\n",
    "\n",
    "# Query from web\n",
    "query = \"1.0,0.0,10,4,307,0,76200,10\"\n",
    "\n",
    "# Split to values\n",
    "values = query.split(\",\")\n",
    "\n",
    "# Prepare dictionary for feature dataframe from web form values\n",
    "features_dict = [{\n",
    "    \"level_index\": float(values[0]),\n",
    "    \"gender_index\": float(values[1]), \n",
    "    \"thumbs_up_sum\": int(values[2]),\n",
    "    \"thumbs_down_sum\": int(values[3]),\n",
    "    \"nextsong_sum\": int(values[4]),\n",
    "    \"downgrade_sum\": int(values[5]),\n",
    "    \"length_sum\": float(values[6]),\n",
    "    \"sessionId_count\": int(values[7]),\n",
    "    }]\n",
    "\n",
    "# Create a user row to use in VectorAssembler\n",
    "df_user_row = spark.createDataFrame(features_dict)\n",
    "\n",
    "# Create feature dataframe with VectorAssembler\n",
    "df_features = VectorAssembler(inputCols = \\\n",
    "                         [\"level_index\", \"gender_index\", \"thumbs_up_sum\", \"thumbs_down_sum\", \\\n",
    "                          \"nextsong_sum\", \"downgrade_sum\", \"length_sum\", \"sessionId_count\"], \\\n",
    "                         outputCol = \"features\").transform(df_user_row)\n",
    "\n",
    "# Select features\n",
    "df_features = df_features.select(\"features\")\n",
    "\n",
    "# Predict on model\n",
    "prediction = model.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[1.0,0.0,10.0,4.0...|[1.73699945213100...|[0.96993883589948...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show result\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.select(\"prediction\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the notebook to an html file\n",
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'web_pred.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
