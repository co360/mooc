{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model\n",
    "\n",
    "Train the best model on the bigger dataset and evaluate once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creating spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('sparkify-capstone-model').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for modelling, tuning and evaluation\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for visualization and output\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "conf.set(\"spark.driver.maxResultSize\",  \"0\")\n",
    "path = \"out/features.parquet\"\n",
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubset(df, factor):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        df: The dataset to split\n",
    "        factor: How much of the dataset to return\n",
    "    OUTPUT: \n",
    "        df_subset: The split subset\n",
    "    \"\"\"\n",
    "    df_subset, df_dummy = df.randomSplit([factor, 1 - factor])\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(tp, fp, tn, fn):\n",
    "    \"\"\" Simple function to output a confusion matrix from f/t/n/p values as html table.\n",
    "    INPUT:\n",
    "        data: The array to print as table\n",
    "    OUTPUT:\n",
    "        Prints the array as html table.\n",
    "    \"\"\"\n",
    "    html = \"<table><tr><td></td><td>Act. True</td><td>False</td></tr>\"\n",
    "    html += \"<tr><td>Pred. Pos.</td><td>{}</td><td>{}</td></tr>\".format(tp, fp)\n",
    "    html += \"<tr><td>Negative</td><td>{}</td><td>{}</td></tr>\".format(fn, tn)    \n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))    \n",
    "    \n",
    "def showEvaluationMetrics(predictions):\n",
    "    \"\"\" Calculate and print the some evaluation metrics for the passed predictions.\n",
    "    INPUT:\n",
    "        predictions: The predictions to evaluate and print\n",
    "    OUTPUT:\n",
    "        Just prints the evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate true, false positives and negatives to calculate further metrics later:\n",
    "    tp = predictions[(predictions.churn == 1) & (predictions.prediction == 1)].count()\n",
    "    tn = predictions[(predictions.churn == 0) & (predictions.prediction == 0)].count()\n",
    "    fp = predictions[(predictions.churn == 0) & (predictions.prediction == 1)].count()\n",
    "    fn = predictions[(predictions.churn == 1) & (predictions.prediction == 0)].count()\n",
    "    \n",
    "    printConfusionMatrix(tp, fp, tn, fn)\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    f1 = MulticlassClassificationEvaluator(labelCol = \"churn\", metricName = \"f1\") \\\n",
    "        .evaluate(predictions)\n",
    "    accuracy = float((tp + tn) / (tp + tn + fp + fn))\n",
    "    recall = float(tp / (tp + fn))\n",
    "    precision = float(tp / (tp + fp))\n",
    "    print(\"F1: \", f1) \n",
    "    print(\"Accuracy: \", accuracy) \n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Precision: \", precision) \n",
    "    \n",
    "def printAUC(predictions, labelCol = \"churn\"):\n",
    "    \"\"\" Print the area under curve for the predictions.\n",
    "    INPUT: \n",
    "        predictions: The predictions to get and print the AUC for\n",
    "    OUTPU:\n",
    "        Prints the AUC\n",
    "    \"\"\"\n",
    "    print(\"Area under curve: \", BinaryClassificationEvaluator(labelCol = labelCol).evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampleNegatives(df, ratio, labelCol = \"churn\"):\n",
    "    \"\"\"\n",
    "    Undersample the negatives (0's) in the given dataframe by ratio.\n",
    "    \n",
    "    NOTE: The \"selection\" method here is of course very crude and in a real version should be randomized and shuffled.\n",
    "    \n",
    "    INPUT:\n",
    "        df: dataframe to undersample negatives from\n",
    "        ratio: Undersampling ratio\n",
    "        labelCol: LAbel column name in the input dataframe\n",
    "    OUTPUT:\n",
    "        A new dataframe with negatives undersampled by ratio\n",
    "    \"\"\"\n",
    "    zeros = df.filter(df[labelCol] == 0)\n",
    "    ones = df.filter(df[labelCol] == 1)\n",
    "    zeros = createSubset(zeros, ratio)\n",
    "    return zeros.union(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbtPredictions(df_train, df_test, maxIter = 10, labelCol = \"churn\", featuresCol = \"features\"):\n",
    "    \"\"\" Fit, evaluate and show results for GBTClassifier \n",
    "    INPUT:\n",
    "        df_train: The training data set.\n",
    "        df_test: The testing data set.\n",
    "        maxIter: Number of maximum iterations in the gradeint boost.\n",
    "        labelCol: The label column name, \"churn\" by default.\n",
    "        featuresCol: The label column name, \"features\" by default.\n",
    "    OUTPUT:\n",
    "        predictions: The model's predictions\n",
    "    \"\"\"\n",
    "    # Fit and train model\n",
    "    gbt = GBTClassifier(labelCol = labelCol, featuresCol = featuresCol, maxIter = maxIter).fit(df_train)\n",
    "    return gbt.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit([0.9, 0.1])\n",
    "\n",
    "gbt = GBTClassifier(labelCol = \"churn\", featuresCol = \"features\", maxIter = 120, maxDepth = 5).fit(undersampleNegatives(df_train, .7))\n",
    "predictions = gbt.transform(df_test)\n",
    "                                 \n",
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt.save(\"out/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the notebook to an html file\n",
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'final-model.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
