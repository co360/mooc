{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web data\n",
    "\n",
    "In this notebook we create a list of userId and churn predictions.\n",
    "This will be displayed in the web app, it wouldn't make sense to calculate this in the web app.\n",
    "\n",
    "We use the previously saved test dataframe to create predictions on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creating spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('sparkify-capstone-web-data').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creating the web data\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "path = \"out/transformed.parquet\"\n",
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model\n",
    "\n",
    "Load the model, prepare for transformation and show some records to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GBTClassificationModel.load(\"out/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GBTClassificationModel (uid=GBTClassifier_4a53f6bf6bbe) with 120 trees"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since values are normalized we can shrink down to unique users\n",
    "df = df.dropDuplicates([\"userId\"])\n",
    "\n",
    "# Drop churn column we will predict this\n",
    "df = df.drop(\"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------+-------------+---------------+------------+-------------+------------------+---------------+\n",
      "|userId|level_index|gender_index|thumbs_up_sum|thumbs_down_sum|nextsong_sum|downgrade_sum|        length_sum|sessionId_count|\n",
      "+------+-----------+------------+-------------+---------------+------------+-------------+------------------+---------------+\n",
      "|   148|        1.0|         0.0|           10|              4|         307|            0| 76207.76939999999|             10|\n",
      "|200049|        1.0|         0.0|            3|              3|          26|            0|        7730.19557|              3|\n",
      "|300040|        1.0|         1.0|           52|             10|         524|            0| 130921.5059899999|              8|\n",
      "|    85|        1.0|         0.0|          116|             33|        2223|           22| 550835.0803399995|             32|\n",
      "|   137|        1.0|         0.0|            2|              0|          49|            0|11998.723940000002|              1|\n",
      "|   251|        1.0|         0.0|          117|             16|        2072|           15|512983.30199999973|             23|\n",
      "|200031|        1.0|         0.0|            2|              2|          27|            0| 6578.325700000001|              1|\n",
      "|300044|        0.0|         0.0|           82|              9|         876|            6|216143.79894000018|              9|\n",
      "|    65|        0.0|         0.0|           89|             15|        1782|           21| 442823.8124299994|             27|\n",
      "|200001|        1.0|         0.0|           19|             11|         295|            0| 75602.64822999998|             10|\n",
      "|    53|        0.0|         0.0|          110|             17|        1833|           27|449392.22696999967|             16|\n",
      "|   255|        0.0|         0.0|           90|             21|        2147|           24| 531555.3366099991|             12|\n",
      "|   133|        1.0|         0.0|            5|              1|          44|            0|       10421.53107|              1|\n",
      "|   296|        1.0|         1.0|            8|              0|         112|            1|28314.150060000004|              5|\n",
      "|100003|        1.0|         1.0|           16|             10|         661|           13|162292.65049999993|              9|\n",
      "|200021|        1.0|         1.0|           51|             45|        1257|           24| 313583.7676999999|             16|\n",
      "|    78|        1.0|         1.0|           66|             16|        1266|           12| 310521.6268000002|             16|\n",
      "|100007|        0.0|         1.0|           40|              6|         853|           18| 210102.7576700001|             12|\n",
      "|100042|        1.0|         1.0|           12|             11|         271|            3| 68557.38241000002|              5|\n",
      "|100035|        0.0|         1.0|           29|              8|         727|           13|180263.01069000005|              8|\n",
      "+------+-----------+------------+-------------+---------------+------------+-------------+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'int'),\n",
       " ('level_index', 'double'),\n",
       " ('gender_index', 'double'),\n",
       " ('thumbs_up_sum', 'bigint'),\n",
       " ('thumbs_down_sum', 'bigint'),\n",
       " ('nextsong_sum', 'bigint'),\n",
       " ('downgrade_sum', 'bigint'),\n",
       " ('length_sum', 'double'),\n",
       " ('sessionId_count', 'bigint')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the prediction dataframe\n",
    "\n",
    "We will calculate the probability and predictions for each user.\n",
    "This could have probably been done much easier if I left the user id in the feature set. But I didn't know\n",
    "back then that the algorithm can choose the input columns. Anyway this is a good excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a struct to create the new dataframe.\n",
    "schema = StructType([\n",
    "    StructField(\"prediction\", DoubleType(), False),\n",
    "    StructField(\"userId\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "df_predictions = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a udf was a bit tricky, when accessing df or model, some thread lock exceptions were thrown.\n",
    "# Using a broadcasted variable did not help. So now we iterate over each row, create the feature vector with VectorAssembler,\n",
    "# make the prediction and add the row to the new dataframe.\n",
    "\n",
    "# Iterate user rows\n",
    "for row in df.toPandas().iterrows():\n",
    "\n",
    "    # Get userId\n",
    "    userId = lit(row[1][0])\n",
    "\n",
    "    # Prepare dictionary for feature dataframe\n",
    "    features_dict = [{\n",
    "        \"level_index\": float(row[1][1]),\n",
    "        \"gender_index\": float(row[1][2]), \n",
    "        \"thumbs_up_sum\": int(row[1][3]),\n",
    "        \"thumbs_down_sum\": int(row[1][4]),\n",
    "        \"nextsong_sum\": int(row[1][5]),\n",
    "        \"downgrade_sum\": int(row[1][6]),\n",
    "        \"length_sum\": float(row[1][7]),\n",
    "        \"sessionId_count\": int(row[1][8]),\n",
    "    }]\n",
    "    \n",
    "    df_user_row = spark.createDataFrame(features_dict)\n",
    "    \n",
    "    # Create feature dataframe with VectorAssembler\n",
    "    df_features = VectorAssembler(inputCols = \\\n",
    "                         [\"level_index\", \"gender_index\", \"thumbs_up_sum\", \"thumbs_down_sum\", \\\n",
    "                          \"nextsong_sum\", \"downgrade_sum\", \"length_sum\", \"sessionId_count\"], \\\n",
    "                         outputCol = \"features\").transform(df_user_row)\n",
    "    \n",
    "    # We now only need the features\n",
    "    df_features = df_features.select(\"features\")\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.transform(df_features)\n",
    "    # Add userId\n",
    "    prediction = prediction.withColumn(\"userId\", userId)\n",
    "    \n",
    "    # Drop unneeded columns\n",
    "    prediction = prediction.drop(\"features\")\n",
    "    prediction = prediction.drop(\"rawPrediction\")\n",
    "    prediction = prediction.drop(\"probability\")\n",
    "    \n",
    "    # Add to new dataframe\n",
    "    df_predictions = df_predictions.union(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|  userId|\n",
      "+----------+--------+\n",
      "|       0.0|   148.0|\n",
      "|       0.0|200049.0|\n",
      "|       0.0|300040.0|\n",
      "|       1.0|    85.0|\n",
      "|       0.0|   137.0|\n",
      "|       0.0|   251.0|\n",
      "|       0.0|200031.0|\n",
      "|       0.0|300044.0|\n",
      "|       0.0|    65.0|\n",
      "|       0.0|200001.0|\n",
      "|       0.0|    53.0|\n",
      "|       0.0|   255.0|\n",
      "|       0.0|   133.0|\n",
      "|       1.0|   296.0|\n",
      "|       1.0|100003.0|\n",
      "|       1.0|200021.0|\n",
      "|       0.0|    78.0|\n",
      "|       0.0|100007.0|\n",
      "|       1.0|100042.0|\n",
      "|       0.0|100035.0|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to parquet file for web app\n",
    "df_predictions.write.parquet(\"out/predictions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the notebook to an html file\n",
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'web.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
