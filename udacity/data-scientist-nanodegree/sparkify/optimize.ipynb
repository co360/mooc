{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final optimizations\n",
    "\n",
    "We will perform a small optimization on some parameters and try different undersampling ratios as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creating spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('sparkify-capstone-model').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for modelling, tuning and evaluation\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for visualization and output\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "conf.set(\"spark.driver.maxResultSize\",  \"0\")\n",
    "path = \"out/features.parquet\"\n",
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubset(df, factor):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        df: The dataset to split\n",
    "        factor: How much of the dataset to return\n",
    "    OUTPUT: \n",
    "        df_subset: The split subset\n",
    "    \"\"\"\n",
    "    df_subset, df_dummy = df.randomSplit([factor, 1 - factor])\n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printConfusionMatrix(tp, fp, tn, fn):\n",
    "    \"\"\" Simple function to output a confusion matrix from f/t/n/p values as html table.\n",
    "    INPUT:\n",
    "        data: The array to print as table\n",
    "    OUTPUT:\n",
    "        Prints the array as html table.\n",
    "    \"\"\"\n",
    "    html = \"<table><tr><td></td><td>Act. True</td><td>False</td></tr>\"\n",
    "    html += \"<tr><td>Pred. Pos.</td><td>{}</td><td>{}</td></tr>\".format(tp, fp)\n",
    "    html += \"<tr><td>Negative</td><td>{}</td><td>{}</td></tr>\".format(fn, tn)    \n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))    \n",
    "    \n",
    "def showEvaluationMetrics(predictions):\n",
    "    \"\"\" Calculate and print the some evaluation metrics for the passed predictions.\n",
    "    INPUT:\n",
    "        predictions: The predictions to evaluate and print\n",
    "    OUTPUT:\n",
    "        Just prints the evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate true, false positives and negatives to calculate further metrics later:\n",
    "    tp = predictions[(predictions.churn == 1) & (predictions.prediction == 1)].count()\n",
    "    tn = predictions[(predictions.churn == 0) & (predictions.prediction == 0)].count()\n",
    "    fp = predictions[(predictions.churn == 0) & (predictions.prediction == 1)].count()\n",
    "    fn = predictions[(predictions.churn == 1) & (predictions.prediction == 0)].count()\n",
    "    \n",
    "    printConfusionMatrix(tp, fp, tn, fn)\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    f1 = MulticlassClassificationEvaluator(labelCol = \"churn\", metricName = \"f1\") \\\n",
    "        .evaluate(predictions)\n",
    "    accuracy = float((tp + tn) / (tp + tn + fp + fn))\n",
    "    recall = float(tp / (tp + fn))\n",
    "    precision = float(tp / (tp + fp))\n",
    "    print(\"F1: \", f1) \n",
    "    print(\"Accuracy: \", accuracy) \n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Precision: \", precision) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampleNegatives(df, ratio, labelCol = \"churn\"):\n",
    "    \"\"\"\n",
    "    Undersample the negatives (0's) in the given dataframe by ratio.\n",
    "    \n",
    "    NOTE: The \"selection\" method here is of course very crude and in a real version should be randomized and shuffled.\n",
    "    \n",
    "    INPUT:\n",
    "        df: dataframe to undersample negatives from\n",
    "        ratio: Undersampling ratio\n",
    "        labelCol: LAbel column name in the input dataframe\n",
    "    OUTPUT:\n",
    "        A new dataframe with negatives undersampled by ratio\n",
    "    \"\"\"\n",
    "    zeros = df.filter(df[labelCol] == 0)\n",
    "    ones = df.filter(df[labelCol] == 1)\n",
    "    zeros = createSubset(zeros, ratio)\n",
    "    return zeros.union(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_gs_GBT(df_train, df_test, labelCol = \"churn\", featuresCol = \"features\"):\n",
    "    \n",
    "    gbt = GBTClassifier(labelCol = labelCol, featuresCol = featuresCol)\n",
    "        \n",
    "    parameterGrid = ParamGridBuilder() \\\n",
    "            .addGrid(gbt.maxDepth, [2, 5]) \\\n",
    "            .addGrid(gbt.maxIter, [30, 120]) \\\n",
    "            .build()\n",
    "    \n",
    "    tv_split = TrainValidationSplit(estimator = gbt, \n",
    "                          estimatorParamMaps = parameterGrid,\n",
    "                          evaluator = MulticlassClassificationEvaluator(labelCol = labelCol),\n",
    "                          trainRatio = 0.8)\n",
    "    \n",
    "    model = tv_split.fit(df_train)    \n",
    "    bestModel = model.bestModel\n",
    "             \n",
    "    bestParams = {'maxDepth':bestModel._java_obj.getMaxDepth(), \\\n",
    "                  'maxIter':bestModel._java_obj.getMaxIter()}\n",
    "        \n",
    "    predictions = model.transform(df_test)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol = labelCol)\n",
    "    \n",
    "    showEvaluationMetrics(predictions)  \n",
    "                     \n",
    "    return bestParams, bestModel\n",
    "\n",
    "# Perform cross-validation and grid search on a small subset\n",
    "df_subset = createSubset(df, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>1012</td><td>3</td></tr><tr><td>Negative</td><td>7</td><td>2553</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.9972011402692175\n",
      "Accuracy:  0.9972027972027973\n",
      "Recall:  0.9931305201177625\n",
      "Precision:  0.9970443349753695\n",
      "Best set of parameters:  {'maxDepth': 5, 'maxIter': 120}\n"
     ]
    }
   ],
   "source": [
    "df_undersampled = undersampleNegatives(df_subset, .6)\n",
    "df_train, df_test = df_undersampled.randomSplit([0.9, 0.1])\n",
    "gbt_bestParams, gbt_bestModel = tv_gs_GBT(df_train, df_test)\n",
    "print(\"Best set of parameters: \", gbt_bestParams)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>953</td><td>1</td></tr><tr><td>Negative</td><td>5</td><td>2043</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.998000218654461\n",
      "Accuracy:  0.9980013324450366\n",
      "Recall:  0.9947807933194155\n",
      "Precision:  0.9989517819706499\n",
      "Best set of parameters:  {'maxDepth': 5, 'maxIter': 120}\n"
     ]
    }
   ],
   "source": [
    "df_undersampled = undersampleNegatives(df_subset, .5)\n",
    "df_train, df_test = df_undersampled.randomSplit([0.9, 0.1])\n",
    "gbt_bestParams, gbt_bestModel = tv_gs_GBT(df_train, df_test)\n",
    "print(\"Best set of parameters: \", gbt_bestParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the notebook to an html file\n",
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'optimize.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
