{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "In this notebook we read in the previously created dataset with features engineered, and then perform the following tasks:\n",
    "    \n",
    "    * Split into train and test data. Also use reduced size subsets for quicker evaluation.\n",
    "    * Evaluate some classification algorithms and paramters.\n",
    "    * Perform simple cross validation to find the best model.\n",
    "    * Measure the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creating spark session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('sparkify-capstone-model').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for modelling, tuning and evaluation\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for visualization and output\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "conf.set(\"spark.driver.maxResultSize\",  \"0\")\n",
    "path = \"out/features.parquet\"\n",
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset set rows/cols: 528005,2\n"
     ]
    }
   ],
   "source": [
    "# Look at some values about the data to confirm it was read in correctly\n",
    "print(\"Dataset set rows/cols: {},{}\".format(df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- churn: integer (nullable = true)\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|churn|\n",
      "+--------------------+-----+\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "|[0.0,0.0,90.0,12....|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "Try three ways to split the data to have an initial idea about how some algorithms perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105582"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we are going to use just a subset of the dataset becuase doing a lot of tuning and cross validation \n",
    "# would take too long otherwise.\n",
    "def createSubset(df, factor):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        df: The dataset to split\n",
    "        factor: How much of the dataset to return\n",
    "    OUTPUT: \n",
    "        df_subset: The split subset\n",
    "    \"\"\"\n",
    "    df_subset, df_dummy = df.randomSplit([factor, 1 - factor])\n",
    "    return df_subset\n",
    "\n",
    "df_subset = createSubset(df, .2)\n",
    "df_subset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94867\n",
      "10715\n"
     ]
    }
   ],
   "source": [
    "# Now we split the subset into train and test. \n",
    "# Note: Best split factor of 90% was determined by trial and error.\n",
    "df_train, df_test = df_subset.randomSplit([0.9, 0.1])\n",
    "print(df_train.count())\n",
    "print(df_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm selection\n",
    "\n",
    "We will be looking at some of pysparks classificatoin algorhtims:\n",
    "\n",
    "    * Logistic Regression (single)\n",
    "    * Random Forest (parallel)\n",
    "    * Gradient-Boosted Tree (sequential)\n",
    "    \n",
    "The most basic, logistic regression is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression\n",
    "\n",
    "First we will be creating a function to fit and train the LR model, which we may use many times.\n",
    "Then we also create some helpful functions for showing the evaluation of the model with some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegressionPredictions(df_train, df_test, threshold = 0.5, labelCol = \"churn\", featuresCol = \"features\"):\n",
    "    \"\"\" Fit, evaluate and show results for LogisticRegression \n",
    "    INPUT:\n",
    "        df_train: The training data set\n",
    "        df_test: The testing data set\n",
    "        threshold: The algorithm's threshold for classification.\n",
    "        labelCol: The label column name, \"churn\" by default.\n",
    "        featuresCol: The label column name, \"features\" by default.\n",
    "    OUTPUT:\n",
    "        predictions: The model's predictions\n",
    "    \"\"\"\n",
    "    # Fit and train model\n",
    "    logreg = LogisticRegression(labelCol = labelCol, featuresCol = featuresCol, threshold = threshold).fit(df_train)\n",
    "    return logreg.evaluate(df_test).predictions\n",
    "\n",
    "def printConfusionMatrix(tp, fp, tn, fn):\n",
    "    \"\"\" Simple function to output a confusion matrix from f/t/n/p values as html table.\n",
    "    INPUT:\n",
    "        data: The array to print as table\n",
    "    OUTPUT:\n",
    "        Prints the array as html table.\n",
    "    \"\"\"\n",
    "    html = \"<table><tr><td></td><td>Act. True</td><td>False</td></tr>\"\n",
    "    html += \"<tr><td>Pred. Pos.</td><td>{}</td><td>{}</td></tr>\".format(tp, fp)\n",
    "    html += \"<tr><td>Negative</td><td>{}</td><td>{}</td></tr>\".format(fn, tn)    \n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))    \n",
    "    \n",
    "def showEvaluationMetrics(predictions):\n",
    "    \"\"\" Calculate and print the some evaluation metrics for the passed predictions.\n",
    "    INPUT:\n",
    "        predictions: The predictions to evaluate and print\n",
    "    OUTPUT:\n",
    "        Just prints the evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate true, false positives and negatives to calculate further metrics later:\n",
    "    tp = predictions[(predictions.churn == 1) & (predictions.prediction == 1)].count()\n",
    "    tn = predictions[(predictions.churn == 0) & (predictions.prediction == 0)].count()\n",
    "    fp = predictions[(predictions.churn == 0) & (predictions.prediction == 1)].count()\n",
    "    fn = predictions[(predictions.churn == 1) & (predictions.prediction == 0)].count()\n",
    "    \n",
    "    printConfusionMatrix(tp, fp, tn, fn)\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    f1 = MulticlassClassificationEvaluator(labelCol = \"churn\", metricName = \"f1\") \\\n",
    "        .evaluate(predictions)\n",
    "    accuracy = float((tp + tn) / (tp + tn + fp + fn))\n",
    "    recall = float(tp / (tp + fn))\n",
    "    precision = float(tp / (tp + fp))\n",
    "    print(\"F1: \", f1) \n",
    "    print(\"Accuracy: \", accuracy) \n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"Precision: \", precision) \n",
    "    \n",
    "def plotROC(df_model):\n",
    "    \"\"\"\n",
    "    Plot the Receiver Operator Curve for the evaluated model\n",
    "    INPUT:\n",
    "        df_model: The trained model\n",
    "    OUTPUT:\n",
    "        Plots the curve\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.plot(df_model.summary.roc.select('FPR').collect(),\n",
    "             df_model.summary.roc.select('TPR').collect())\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.show()    \n",
    "    \n",
    "def printAUC(predictions, labelCol = \"churn\"):\n",
    "    \"\"\" Print the area under curve for the predictions.\n",
    "    INPUT: \n",
    "        predictions: The predictions to get and print the AUC for\n",
    "    OUTPU:\n",
    "        Prints the AUC\n",
    "    \"\"\"\n",
    "    print(\"Area under curve: \", BinaryClassificationEvaluator(labelCol = labelCol).evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>236</td><td>107</td></tr><tr><td>Negative</td><td>1845</td><td>8527</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.7608456698702306\n",
      "Accuracy:  0.8178254783014466\n",
      "Recall:  0.1134070158577607\n",
      "Precision:  0.6880466472303207\n"
     ]
    }
   ],
   "source": [
    "predictions = logisticRegressionPredictions(df_train, df_test)    \n",
    "showEvaluationMetrics(predictions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under curve:  0.6775022076149889\n"
     ]
    }
   ],
   "source": [
    "# Evaluation results:\n",
    "\n",
    "# The confusion matrix looks pretty good but a bit worrying is the high count of false negatives (Type 2 errors).\n",
    "# This is possibly due to our label set being unbalanced.\n",
    "\n",
    "# F1 score is looking good, I can be happy with over 80% at first try.\n",
    "# But as expected, the high false negative count lead to a bad recall score.\n",
    "# In the case of churn, a false negative is not a desirable prediction outcome, because this means we miss customers\n",
    "# who actually churn and do not get a chance to change their minds. Which is the point of the excersize.\n",
    "# So we will try to tune this and also watch the overall score.\n",
    "\n",
    "# But before, let's plot the ROC and show AUC:\n",
    "\n",
    "model = LogisticRegression(labelCol = \"churn\").fit(df_train)\n",
    "model.evaluate(df_test).predictions\n",
    "\n",
    "plotROC(model)\n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks like a lot of RO curves I have seen so that and also high AUC seems promising as well.\n",
    "# Lookig the the curve though, we see there is some room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: Threshold\n",
    "\n",
    "Since our input data is skewed on the label, we can try to undersample negative cases by lowering the threshold.\n",
    "Hopefully, this will help significantly with our bad recall rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>261</td><td>167</td></tr><tr><td>Negative</td><td>1820</td><td>8467</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.7615725199332655\n",
      "Accuracy:  0.8145590293980401\n",
      "Recall:  0.12542047092743874\n",
      "Precision:  0.6098130841121495\n"
     ]
    }
   ],
   "source": [
    "predictions = logisticRegressionPredictions(df_train, df_test, 0.4)    \n",
    "showEvaluationMetrics(predictions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>236</td><td>131</td></tr><tr><td>Negative</td><td>1845</td><td>8503</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.7593514772947256\n",
      "Accuracy:  0.815585627624825\n",
      "Recall:  0.1134070158577607\n",
      "Precision:  0.6430517711171662\n"
     ]
    }
   ],
   "source": [
    "# We see that our f1 score has slightly improved. Recall increased by quite a bit but precision also went down.\n",
    "# Let's try one more time with an only slighty smaller threshold:\n",
    "predictions = logisticRegressionPredictions(df_train, df_test, 0.45)    \n",
    "showEvaluationMetrics(predictions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall went down and precision up.\n",
    "# So there is a tradeoff here. Maybe if we change the input data itself? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: Undersampling negatives in the input data\n",
    "\n",
    "As an alternative to penalize false negatives, we can try to undersample them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones: 20658, Zeros: 84924\n",
      "24.325279073053554\n"
     ]
    }
   ],
   "source": [
    "# Check distributions of churn \n",
    "zeros = df_subset.filter(df[\"churn\"] == 0)\n",
    "ones = df_subset.filter(df[\"churn\"] == 1)\n",
    "zerosCount = zeros.count()\n",
    "onesCount = ones.count()\n",
    "print(\"Ones: {}, Zeros: {}\".format(onesCount, zerosCount))\n",
    "print(onesCount / zerosCount * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a \"quick and dirty\" check, we will just hack off some 0's and see what happens.\n",
    "# Note: Normally we would use something like KStratified sampling or similar but that is beyond the scode of this project.\n",
    "def undersampleNegatives(df, ratio, labelCol = \"churn\"):\n",
    "    \"\"\"\n",
    "    Undersample the negatives (0's) in the given dataframe by ratio.\n",
    "    \n",
    "    NOTE: The \"selection\" method here is of course very crude and in a real version should be randomized and shuffled.\n",
    "    \n",
    "    INPUT:\n",
    "        df: dataframe to undersample negatives from\n",
    "        ratio: Undersampling ratio\n",
    "        labelCol: LAbel column name in the input dataframe\n",
    "    OUTPUT:\n",
    "        A new dataframe with negatives undersampled by ratio\n",
    "    \"\"\"\n",
    "    zeros = df.filter(df[labelCol] == 0)\n",
    "    ones = df.filter(df[labelCol] == 1)\n",
    "    zeros = createSubset(zeros, ratio)\n",
    "    return zeros.union(ones)\n",
    "                                \n",
    "df_undersampled = undersampleNegatives(df_subset, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones: 20658, Zeros: 67911\n",
      "30.41922516234483\n"
     ]
    }
   ],
   "source": [
    "# Check distribution again\n",
    "zeros = df_undersampled.filter(df[\"churn\"] == 0)\n",
    "ones = df_undersampled.filter(df[\"churn\"] == 1)\n",
    "zerosCount = zeros.count()\n",
    "onesCount = ones.count()\n",
    "print(\"Ones: {}, Zeros: {}\".format(onesCount, zerosCount))\n",
    "print(onesCount / zerosCount * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>191</td><td>88</td></tr><tr><td>Negative</td><td>1632</td><td>6087</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.717912749515747\n",
      "Accuracy:  0.7849462365591398\n",
      "Recall:  0.1047723532638508\n",
      "Precision:  0.6845878136200717\n",
      "Area under curve:  0.6747464805310462\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df_undersampled.randomSplit([0.91, 0.09])\n",
    "predictions = logisticRegressionPredictions(df_train, df_test)    \n",
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, our recall score did go up a little, but at the cost of precision.\n",
    "# Same as when we modified te threshold.\n",
    "# The f1 score stayed about the same, but the AUC went up a little.\n",
    "# \n",
    "# So while we do have some small optimization, it will be a business decision to decide which way to \n",
    "# tune the model - either more precision or more recall.\n",
    "# Another thing we can try is to modify our input data in the ETL stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest\n",
    "\n",
    "Logistic regression performed reasonably well but there was a very low recall rate.\n",
    "Let's see how a RandomForesClassifier does on this dataset.\n",
    "Since we have some high correlation, an ensemble learning model\n",
    "like RFC might do a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestPredictions(df_train, df_test, numTrees = 50, labelCol = \"churn\", featuresCol = \"features\"):\n",
    "    \"\"\" Fit, evaluate and show results for RandomForestClassifier \n",
    "    INPUT:\n",
    "        df_train: The training data set.\n",
    "        df_test: The testing data set.\n",
    "        numTrees: Number of trees in the forest.\n",
    "        labelCol: The label column name, \"churn\" by default.\n",
    "        featuresCol: The label column name, \"features\" by default.\n",
    "    OUTPUT:\n",
    "        predictions: The model's predictions.\n",
    "    \"\"\"\n",
    "    # Fit and train model\n",
    "    rfc = RandomForestClassifier(labelCol = labelCol, featuresCol = featuresCol, numTrees = numTrees).fit(df_train)\n",
    "    return rfc.transform(df_test)\n",
    "\n",
    "predictions = randomForestPredictions(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>304</td><td>0</td></tr><tr><td>Negative</td><td>1519</td><td>6175</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.7526614325181695\n",
      "Accuracy:  0.810077519379845\n",
      "Recall:  0.16675809105869446\n",
      "Precision:  1.0\n",
      "Area under curve:  0.9223249926157222\n"
     ]
    }
   ],
   "source": [
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While there is a very low recall rate, the Precision is perfect (This could be due to our reduced dataset size), \n",
    "# and the AUC is pretty high as well.\n",
    "# Let's try another optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: numTrees\n",
    "\n",
    "We will try to increase the number of decision trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>304</td><td>0</td></tr><tr><td>Negative</td><td>1519</td><td>6175</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.7526614325181695\n",
      "Accuracy:  0.810077519379845\n",
      "Recall:  0.16675809105869446\n",
      "Precision:  1.0\n",
      "Area under curve:  0.9212026268041512\n"
     ]
    }
   ],
   "source": [
    "predictions = randomForestPredictions(df_train, df_test, 100)\n",
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not a whole lot has improved.\n",
    "# Lastly we try to run the RF on the undersampled input data.\n",
    "# Since it severly penalized the recall, we will remove a larger portion of negatives this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: Undersampled negatives\n",
    "\n",
    "Same as above with LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>1789</td><td>506</td></tr><tr><td>Negative</td><td>270</td><td>1782</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.8214704759112346\n",
      "Accuracy:  0.8214860823556476\n",
      "Recall:  0.8688683827100534\n",
      "Precision:  0.779520697167756\n",
      "Area under curve:  0.9296603560354167\n"
     ]
    }
   ],
   "source": [
    "# ratio was derived by trial and error\n",
    "df_undersampled = undersampleNegatives(df_subset, .264)\n",
    "df_train, df_test = df_undersampled.randomSplit([0.9, 0.1])\n",
    "predictions = randomForestPredictions(df_train, df_test, 100)\n",
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>1851</td><td>1879</td></tr><tr><td>Negative</td><td>272</td><td>6747</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.8170823070494587\n",
      "Accuracy:  0.7998883617080659\n",
      "Recall:  0.8718794159208667\n",
      "Precision:  0.49624664879356567\n",
      "Area under curve:  0.9288822616591779\n"
     ]
    }
   ],
   "source": [
    "# It seems that with severely reduced negatives in the data, the RandomForest classifier was able to converge better.\n",
    "# The f1 score has worsened, but now recall is on a higher level, meaning we would not miss\n",
    "# as many customers churning as before.\n",
    "# But will these good results from the undersampled data hold for the real test data? Let's see:\n",
    "\n",
    "rfc = RandomForestClassifier(labelCol = \"churn\", featuresCol = \"features\", numTrees = 100).fit(df_train)\n",
    "df_train, df_test = df_subset.randomSplit([0.9, 0.1])\n",
    "predictions = rfc.transform(df_test)\n",
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we see that there is a tradeoff to be expected between recall and precision.\n",
    "# But the f1 score is good, as is precision, and now recall is looking better, so it's an improvement.\n",
    "# Anyway this concludes the preliminary experiments for RandomForest tuning, in conclusion I like the model better as it seems\n",
    "# to perform better and also gives more leeway when tuning as compared to LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost\n",
    "\n",
    "As last algorithm we try a gradient boosted tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbtPredictions(df_train, df_test, maxIter = 10, labelCol = \"churn\", featuresCol = \"features\"):\n",
    "    \"\"\" Fit, evaluate and show results for GBTClassifier \n",
    "    INPUT:\n",
    "        df_train: The training data set.\n",
    "        df_test: The testing data set.\n",
    "        maxIter: Number of maximum iterations in the gradeint boost.\n",
    "        labelCol: The label column name, \"churn\" by default.\n",
    "        featuresCol: The label column name, \"features\" by default.\n",
    "    OUTPUT:\n",
    "        predictions: The model's predictions\n",
    "    \"\"\"\n",
    "    # Fit and train model\n",
    "    gbt = GBTClassifier(labelCol = labelCol, featuresCol = featuresCol, maxIter = maxIter).fit(df_train)\n",
    "    return gbt.transform(df_test)\n",
    "\n",
    "predictions = gbtPredictions(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>1225</td><td>0</td></tr><tr><td>Negative</td><td>898</td><td>8626</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.9073201489745509\n",
      "Accuracy:  0.9164573448692902\n",
      "Recall:  0.5770136599152144\n",
      "Precision:  1.0\n",
      "Area under curve:  0.9912217813817268\n"
     ]
    }
   ],
   "source": [
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GBT results look very promising. There are very high rates right off the bat, and recall is nearing 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: Unersampling negatives\n",
    "\n",
    "With our experience from above, let's try undersampling as first optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>1500</td><td>14</td></tr><tr><td>Negative</td><td>589</td><td>5025</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.9109367889590346\n",
      "Accuracy:  0.9154040404040404\n",
      "Recall:  0.7180469123982767\n",
      "Precision:  0.9907529722589168\n",
      "Area under curve:  0.99112660833816\n"
     ]
    }
   ],
   "source": [
    "# What happens here is largeley analogous to similar procedures above.\n",
    "df_undersampled = undersampleNegatives(df_subset, .6)\n",
    "df_train, df_test = df_undersampled.randomSplit([0.9, 0.1])\n",
    "predictions = gbtPredictions(df_train, df_test)\n",
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>1548</td><td>18</td></tr><tr><td>Negative</td><td>586</td><td>8422</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.9394240768846722\n",
      "Accuracy:  0.9428787592207301\n",
      "Recall:  0.725398313027179\n",
      "Precision:  0.9885057471264368\n",
      "Area under curve:  0.9919879617743863\n"
     ]
    }
   ],
   "source": [
    "# This is a significant improvement. There is a strongly improved recall rate at a high precision.\n",
    "# Let's see how it does on the normal sampled test set:\n",
    "\n",
    "gbt = GBTClassifier(labelCol = \"churn\", featuresCol = \"features\", maxIter = 10).fit(df_train)\n",
    "df_train, df_test = df_subset.randomSplit([0.9, 0.1])\n",
    "\n",
    "predictions = gbt.transform(df_test)\n",
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model was able to translate it's performance very well to the normal sampled dataset. \n",
    "# I am convinced that this is the best algorithm to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full undersampled dataset\n",
    "\n",
    "Let us run and evalute the algorithm on the full (undersampled) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td>Act. True</td><td>False</td></tr><tr><td>Pred. Pos.</td><td>7589</td><td>228</td></tr><tr><td>Negative</td><td>2766</td><td>42569</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.9405484467736669\n",
      "Accuracy:  0.9436709813365443\n",
      "Recall:  0.7328826653790439\n",
      "Precision:  0.9708328003070231\n",
      "Area under curve:  0.9891197861572065\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df.randomSplit([0.9, 0.1])\n",
    "df_undersampled = undersampleNegatives(df_train, .6)\n",
    "\n",
    "gbt = GBTClassifier(labelCol = \"churn\", featuresCol = \"features\", maxIter = 10).fit(df_undersampled)\n",
    "predictions = gbt.transform(df_test)\n",
    "                                 \n",
    "showEvaluationMetrics(predictions)  \n",
    "printAUC(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The f1 score is very good, in fact all scores but recall are very good.\n",
    "# But recall is still very acceptable under these conditions.\n",
    "# Now we can use automated optimization to search for the best GBT model.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the notebook to an html file\n",
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'model.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
