{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web predictions\n",
    "\n",
    "The purpose of this notebook is to experiment with making predictions from \"raw\" accumulated user values, that\n",
    "could for instance be user input from a web form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('sparkify-capstone-web').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedPath = \"out/transformed.parquet\"\n",
    "predictionsPath = \"out/predictions.parquet\"\n",
    "df_transformed = spark.read.parquet(transformedPath)\n",
    "df_predictions = spark.read.parquet(predictionsPath)\n",
    "model = GBTClassificationModel.load(\"out/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones: 93, Zeros: 355\n",
      "26.197183098591548\n"
     ]
    }
   ],
   "source": [
    "zeros = df_predictions.filter(df_predictions[\"prediction\"] == 0)\n",
    "ones = df_predictions.filter(df_predictions[\"prediction\"] == 1)\n",
    "zerosCount = zeros.count()\n",
    "onesCount = ones.count()\n",
    "print(\"Ones: {}, Zeros: {}\".format(onesCount, zerosCount))\n",
    "print(onesCount / zerosCount * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPredictedToChurn = df_predictions.filter(df_predictions[\"prediction\"] == 1).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "296\n",
      "100003\n",
      "200021\n",
      "100042\n"
     ]
    }
   ],
   "source": [
    "for row in usersPredictedToChurn:\n",
    "    print(int(row[\"userId\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+------------+-------------+---------------+------------+-------------+-----------------+---------------+\n",
      "|userId|churn|level_index|gender_index|thumbs_up_sum|thumbs_down_sum|nextsong_sum|downgrade_sum|       length_sum|sessionId_count|\n",
      "+------+-----+-----------+------------+-------------+---------------+------------+-------------+-----------------+---------------+\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        1.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        1.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        1.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        1.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "|    93|    0|        0.0|         0.0|           90|             12|        1628|           16|412840.6919200001|             16|\n",
      "+------+-----+-----------+------------+-------------+---------------+------------+-------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|  userId|\n",
      "+----------+--------+\n",
      "|       0.0|   148.0|\n",
      "|       0.0|200049.0|\n",
      "|       0.0|300040.0|\n",
      "|       1.0|    85.0|\n",
      "|       0.0|   137.0|\n",
      "|       0.0|   251.0|\n",
      "|       0.0|200031.0|\n",
      "|       0.0|300044.0|\n",
      "|       0.0|    65.0|\n",
      "|       0.0|200001.0|\n",
      "|       0.0|    53.0|\n",
      "|       0.0|   255.0|\n",
      "|       0.0|   133.0|\n",
      "|       1.0|   296.0|\n",
      "|       1.0|100003.0|\n",
      "|       1.0|200021.0|\n",
      "|       0.0|    78.0|\n",
      "|       0.0|100007.0|\n",
      "|       1.0|100042.0|\n",
      "|       0.0|100035.0|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 300044\n",
    "# 0 251\n",
    "\n",
    "# Select the prediction of a user as value\n",
    "pred = df_predictions[df_predictions[\"userId\"] == 78].select(\"prediction\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\runtimes\\spark-3.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\session.py:378: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "# From a query that could be entered in a web form, create a prediction\n",
    "\n",
    "# Query from web\n",
    "query = \"1.0,0.0,10,4,307,0,76200,10\"\n",
    "\n",
    "# Split to values\n",
    "values = query.split(\",\")\n",
    "\n",
    "# Prepare dictionary for feature dataframe from web form values\n",
    "features_dict = [{\n",
    "    \"level_index\": float(values[0]),\n",
    "    \"gender_index\": float(values[1]), \n",
    "    \"thumbs_up_sum\": int(values[2]),\n",
    "    \"thumbs_down_sum\": int(values[3]),\n",
    "    \"nextsong_sum\": int(values[4]),\n",
    "    \"downgrade_sum\": int(values[5]),\n",
    "    \"length_sum\": float(values[6]),\n",
    "    \"sessionId_count\": int(values[7]),\n",
    "    }]\n",
    "\n",
    "# Create a user row to use in VectorAssembler\n",
    "df_user_row = spark.createDataFrame(features_dict)\n",
    "\n",
    "# Create feature dataframe with VectorAssembler\n",
    "df_features = VectorAssembler(inputCols = \\\n",
    "                         [\"level_index\", \"gender_index\", \"thumbs_up_sum\", \"thumbs_down_sum\", \\\n",
    "                          \"nextsong_sum\", \"downgrade_sum\", \"length_sum\", \"sessionId_count\"], \\\n",
    "                         outputCol = \"features\").transform(df_user_row)\n",
    "\n",
    "# Select features\n",
    "df_features = df_features.select(\"features\")\n",
    "\n",
    "# Predict on model\n",
    "prediction = model.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[1.0,0.0,10.0,4.0...|[1.59711984191447...|[0.96061692995759...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show result\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.select(\"prediction\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the notebook to an html file\n",
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'web_pred.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
